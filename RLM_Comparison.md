# Recursive Language Model (RLM) Comparison

This document compares the Recursive Language Model (RLM) implementation described in the reference paper/blog post with two local implementations: the lightweight `src/log_agent.py` and the robust MCP-based `aleph_rlm_repl`.

## 1. External Implementation Overview (from `RecursiveLanguageModels.txt`)

**Concept**:  
The external RLM proposes a "Context-Centric" inference strategy designed to handle effectively unbounded context lengths (e.g., 10M+ tokens) without "context rot."

**Key Architecture**:
*   **Root LM (Depth 0)**: The main model receives the user query but *does not* see the full context directly.
*   **Environment (REPL)**: The context is pre-loaded as a variable (e.g., a Python string or object) within a persistent REPL environment.
*   **Recursive Capabilities**: The Root LM can generate code that explicitly calls a **Recursive LM (Depth 1)**.
    *   *Example*: `answer = rlm_call(chunk_of_context)`
*   **Workflow**:
    1.  The Root LM "peeks" at the data (e.g., `print(context[:1000])`).
    2.  It uses standard Python tools (grep, regex) to filter or partition the data.
    3.  For semantic tasks (e.g., "summarize," "find specific meaning"), it iterates over chunks of the data and calls the Recursive LM on them.
    4.  The results are aggregated programmatically to form the final answer.

**Main Advantage**:  
It enables "Semantic Map-Reduce." The model can perform intelligent operations (summarization, reasoning) on huge datasets by breaking them down and delegating to sub-model calls within the code loop, keeping the Root LM's context window clean.

---

## 2. Local Implementation Overview (`src/log_agent.py`)

**Concept**:  
The local implementation is a **ReAct Agent** (built with DSPy) equipped with a persistent Python REPL tool. It focuses on using Python for accurate calculation and data retrieval.

**Key Architecture**:
*   **Agent (DSPy ReAct)**: A standard ReAct loop that plans and executes tool calls.
*   **Tool (`python_repl`)**: A custom tool that executes Python code and maintains state between calls.
*   **Context Loading**:
    *   Data is not pre-loaded. The agent must call `fetch_log_data()` within the REPL to load file content into a variable.
    *   *Similarity*: If the agent assigns the result to a variable (e.g., `data = fetch_log_data(...)`), the raw data stays in the REPL and does *not* flood the Agent's context window.
*   **Capabilities**:
    *   The REPL allows "safe" imports (standard library like `re`, `json`, `math`).
    *   It exposes specific helper functions (`fetch_log_data`, `get_available_files`).

**Limitation**:  
There is **no recursive LLM call** exposed to the REPL. The Python code generated by the agent can only perform *deterministic* operations (regex, counting, sorting). It cannot say `call_llm(data_chunk)` to interpret text.

---

## 3. Aleph Implementation Overview (`aleph_rlm_repl`)

**Concept**:
Aleph is a Model Context Protocol (MCP) server that implements a production-grade RLM. It acts as an "Unbounded Cognition" engine, treating system RAM as the only limit to context size.

**Key Architecture**:
*   **MCP Server**: Exposes a standardized set of tools (`load_context`, `search_context`, `exec_python`) that any MCP-compliant client (Claude, Cursor, etc.) can use.
*   **Environment (Python Process)**: A persistent process holding gigabytes of data in memory variables.
*   **Recursive Capabilities**: Explicit support via the `sub_query()` function available inside the `exec_python` sandbox.
    *   *Example*: `results = [sub_query("summarize", chunk) for chunk in chunks]`
*   **Workflow**:
    1.  **Load**: `load_context` or `load_file` reads massive data into RAM (not LLM context).
    2.  **Explore**: `search_context` (regex) and `peek_context` (slices) allow efficient navigation.
    3.  **Compute & Recurse**: `exec_python` allows running arbitrary code, including spawning sub-agents via `sub_query` to process data chunks in parallel.
    4.  **Persist**: Sessions can be saved to disk (`save_session`) to resume complex reasoning tasks later.

**Main Advantage**:
It fully realizes the RLM vision with a standardized interface (MCP). It supports "Deep Reasoning Loops" (think -> search -> evaluate) and persistent memory across sessions.

---

## 4. Detailed Comparison

| Feature | External RLM (Paper) | Local (`log_agent.py`) | Aleph (`aleph_rlm_repl`) |
| :--- | :--- | :--- | :--- |
| **Context Storage** | Pre-loaded in REPL variable. | Fetched on-demand into REPL. | Loaded into separate Python Process (RAM). |
| **Context Visibility** | Root LM sees extracts. | Agent sees extracts. | Agent sees extracts (via `search`/`peek`). |
| **Processing Logic** | **Hybrid**: Python + Recursive LM. | **Python Only**: Algorithmic. | **Hybrid**: Python + Recursive `sub_query`. |
| **Recursion** | **Explicit**: `rlm.completion()`. | **None**: No LLM callback. | **Explicit**: `sub_query()` in sandbox. |
| **Architecture** | Research Prototype. | DSPy ReAct Agent. | MCP Server (Standardized). |
| **Persistence** | Session-based. | Runtime only. | Full Session Save/Load (`save_session`). |
| **Use Case** | Deep research, semantic map-reduce. | Precise log analysis/counting. | Unbounded context analysis, multi-session reasoning. |

### Critical Difference: The "Semantic Gap"

*   **Aleph** and the **External RLM** bridge the semantic gap. They can answer *"Find the log entry where the user seemed most frustrated"* by iterating over lines and asking a sub-model to evaluate sentiment.
*   **Local `log_agent.py`** cannot do this. It is limited to exact keyword matching or deterministic logic.

### Conclusion

*   **Local `log_agent.py`** is a specialized *Tool-Use Agent* for deterministic log analysis. It is lightweight and efficient for exact tasks.
*   **Aleph** is a full-featured *Recursive Language Model* system. It provides the infrastructure (MCP, Persistence, Recursion) to handle massive semantic tasks that require breaking down data and processing it with sub-calls. It effectively "solves" the context window problem by treating it as an external memory resource.